model_list:
  - model_name: openai/glm-4-5-air-mlx # letta server bug?
    litellm_params:
      model: openai/glm-4.5-air-mlx
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 260000  
      max_prompt_tokens: 260000
      custom_llm_provider: openai
    model_info:
      id: "glm-4.5-air-mlx"
  - model_name: local-glm-4-5-air-mlx
    litellm_params:
      model: openai/glm-4.5-air-mlx
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 260000  
      max_prompt_tokens: 260000
    model_info:
      id: "glm-4.5-air-mlx"
  - model_name: local-qwen/qwen3-coder-30b
    litellm_params:
      model: openai/qwen/qwen3-coder-30b
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 260000  
      max_prompt_tokens: 260000
    model_info:
      id: "qwen3-coder-30b"
  - model_name: local-qwen/qwen3-next-80b
    litellm_params:
      model: openai/qwen/qwen3-next-80b
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 260000  
      max_prompt_tokens: 260000
    model_info:
      id: "qwen3-next-80b"
  - model_name: local-openai/gpt-oss-20b
    litellm_params:
      model: openai/openai/gpt-oss-20b
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 131000  
      max_prompt_tokens: 131000
    model_info:
      id: "gpt-oss-20b"
  - model_name: local-openai/gpt-oss-120b
    litellm_params:
      model: openai/openai/gpt-oss-120b
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 131000  
      max_prompt_tokens: 131000
    model_info:
      id: "gpt-oss-120b"
  - model_name: GCP-gemma-3-27b
    litellm_params:
      model: gemini/gemma-3-27b
      api_key: os.environ/GEMINI_API_KEY
      context_limit: 1000000  
      max_prompt_tokens: 1000000
  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY
      context_limit: 1000000  
      max_prompt_tokens: 1000000
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY
      context_limit: 1000000  
      max_prompt_tokens: 1000000
  - model_name: bedrock-qwen-qwen3-next-80b-a3b
    litellm_params:
      model: bedrock/converse/qwen.qwen3-next-80b-a3b
      aws_access_key_id: os.environ/aws_access_key_id
      aws_secret_access_key: os.environ/aws_secret_access_key
      aws_region_name: us-east-1  # Or your preferred region
    model_info:
      id: "bedrock/converse/qwen.qwen3-next-80b-a3b"
  - model_name: bedrock-openai-gpt-oss-120b-1-0
    litellm_params:
      model: "bedrock/openai.gpt-oss-120b-1:0"
      aws_access_key_id: os.environ/aws_access_key_id
      aws_secret_access_key: os.environ/aws_secret_access_key
      aws_region_name: us-east-1  # Or your preferred region
    model_info:
      id: "bedrock/openai.gpt-oss-120b-1:0"
  - model_name: bedrock/amazon.titan-embed-text-v1 # New Bedrock embedding model
    litellm_params:
      model: "bedrock/amazon.titan-embed-text-v1" # Full LiteLLM Bedrock model name
      aws_access_key_id: os.environ/aws_access_key_id
      aws_secret_access_key: os.environ/aws_secret_access_key
      aws_region_name: us-east-1
    model_info:
      id: "amazon.titan-embed-text-v1"
  - model_name: openai/local-Nomic-ai/nomic-embed-text-v1-5
    litellm_params:
      model: openai/nomic-embed-text-v1.5
      api_key: "sk-litellm-dummy" # dummy key for local endpoint
      api_base: http://localhost:1234/v1
      custom_llm_provider: openai # Explicitly set provider to avoid Bedrock confusion
  - model_name: local-bge-small-en-v1-5
    litellm_params:
      model: openai/bge-small-en-v1.5
      api_key: "not-needed"
      api_base: http://localhost:1234/v1


litellm_settings:
  timeout: 7200
  stream_timeout: 7200
  # max_budget: 0.001 # not used - may provide false sense of security
  # budget_duration: 1d
  num_retries: 1
  request_timeout: 7200
  drop_params: true # added as part of bedrock first use
  modify_params: true # added as part of bedrock first use

  cache: True
  cache_params:
    type: "redis-semantic"
    similarity_threshold: 0.8
    redis_semantic_cache_embedding_model: bedrock/amazon.titan-embed-text-v1 # use the Bedrock embedding model for semantic cache
    # NOTE: redis_semantic_cache_embedding_model doesn't seem used, but is needed to start the cache
    ttl: 300 # Extend cache lifetime to 5 minutes
    # NOTE: if using agents, this may not be hit regardless
    skip_system_message_in_cache_key: True # More consistent cache keys
  
general_settings:
  ui_features:
    analytics_dashboard: true
    log_requests: true
  store_model_in_db: true
  store_prompts_in_spend_logs: true # NOTE: PII consideration
  master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 30
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
  # background_health_checks: true
  # use_shared_health_check: true
  # health_check_interval: 30
  database_url: "postgresql://postgres@localhost:5432/mylitellm" # [OPTIONAL] use for token-based auth to proxy



# https://docs.litellm.ai/docs/proxy/custom_pricing
# https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
# input_cost_per_token_above_200k_tokens - Cost for input tokens when context exceeds 200k tokens
# output_cost_per_token_above_200k_tokens - Cost for output tokens when context exceeds 200k tokens
# cache_creation_input_token_cost_above_200k_tokens - Cache creation cost for large contexts
# cache_read_input_token_cost_above_200k_token - Cache read cost for large contexts
# input_cost_per_image - Cost per image in multimodal requests
# output_cost_per_reasoning_token - Cost for reasoning tokens (e.g., OpenAI o1 models)
# input_cost_per_audio_token - Cost for audio input tokens
# output_cost_per_audio_token - Cost for audio output tokens
# input_cost_per_video_per_second - Cost per second of video input
# input_cost_per_video_per_second_above_128k_tokens - Video cost for large contexts
# input_cost_per_character