model_list:
  - model_name: qwen/qwen3-coder-30b
    litellm_params:
      model: openai/qwen/qwen3-coder-30b
      api_key: "not-needed"
      api_base: http://localhost:1234/v1
      context_limit: 260000  
      max_prompt_tokens: 260000
    model_info:
      id: "qwen3-coder-30b"



# {
#   "version": 1,
#   "provider": "openai",
#   "model": "qwen/qwen3-coder-30b",
#   "modelParams": {},
#   "ephemeralSettings": {
#     "auth-key": "not-needed",
#     "base-url": "http://localhost:1234/v1",
#     "context-limit": 260000,
#     "disabled-tools": [
#       "google_web_search"
#     ],
#     "max-prompt-tokens": 260000
#   },
#   "permittedShellCommands": ["ls -la", "npm install", "git status"]
# }



litellm_settings:
#   disable_spend_logs: True   # Disable writing spend logs to DB
#   disable_error_logs: True   # Only disable writing error logs to DB, regular spend logs will still be written unless `disable_spend_logs: True`
#   # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
#   drop_params: True
  max_budget: 2
  budget_duration: 1d
  num_retries: 5
  request_timeout: 7200
#   telemetry: False
#   context_window_fallbacks: [{"gpt-3.5-turbo": ["gpt-3.5-turbo-large"]}]
#   default_team_settings: 
#     - team_id: team-1
#       success_callback: ["langfuse"]
#       failure_callback: ["langfuse"]
#       langfuse_public_key: os.environ/LANGFUSE_PROJECT1_PUBLIC # Project 1
#       langfuse_secret: os.environ/LANGFUSE_PROJECT1_SECRET # Project 1
#     - team_id: team-2
#       success_callback: ["langfuse"]
#       failure_callback: ["langfuse"]
#       langfuse_public_key: os.environ/LANGFUSE_PROJECT2_PUBLIC # Project 2
#       langfuse_secret: os.environ/LANGFUSE_PROJECT2_SECRET # Project 2
#       langfuse_host: https://us.cloud.langfuse.com
#   # cache: true   # [OPTIONAL] use for caching responses 
#   # enable_caching_on_provider_specific_optional_params: True  # Include provider-specific params in cache keys
#   # cache_params:  # And for shared health check
#   #   type: redis
#   #   host: localhost
#   #   port: 6379

# # For /fine_tuning/jobs endpoints
# finetune_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# # for /files endpoints
# files_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# router_settings:
#   routing_strategy: usage-based-routing-v2 
#   redis_host: os.environ/REDIS_HOST
#   redis_password: os.environ/REDIS_PASSWORD
#   redis_port: os.environ/REDIS_PORT
#   enable_pre_call_checks: true
#   model_group_alias: {"my-special-fake-model-alias-name": "fake-openai-endpoint-3"} 

general_settings: 
  master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
  # background_health_checks: true
  # use_shared_health_check: true
  # health_check_interval: 30
  database_url: "postgresql://postgres@localhost:5432/mylitellm" # [OPTIONAL] use for token-based auth to proxy

#   pass_through_endpoints:
#     - path: "/v1/rerank"                                  # route you want to add to LiteLLM Proxy Server
#       target: "https://api.cohere.com/v1/rerank"          # URL this route should forward requests to
#       headers:                                            # headers to forward to this URL
#         content-type: application/json                    # (Optional) Extra Headers to pass to this endpoint 
#         accept: application/json
#       forward_headers: True

# # environment_variables:
#   # settings for using redis caching
#   # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
#   # REDIS_PORT: "16337"
#   # REDIS_PASSWORD: 
